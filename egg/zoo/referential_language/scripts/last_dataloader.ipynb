{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parent Class for a VG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path: str) -> Image.Image:\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "class VisualGenomeDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        metadata_dir: str,\n",
    "        classes_path: str = \"/private/home/rdessi/EGG/egg/zoo/referential_language/utils/classes_1600.txt\",\n",
    "        split: str = \"train\",\n",
    "        transform: Callable = transforms.ToTensor(),\n",
    "        max_objects=20,\n",
    "        image_size=64,\n",
    "    ):\n",
    "        path_images = Path(image_dir)\n",
    "        path_metadata = Path(metadata_dir) / f\"{split}_objects.json\"\n",
    "        path_image_data = Path(metadata_dir) / f\"{split}_image_data.json\"\n",
    "\n",
    "        with open(path_image_data) as img_in, open(path_metadata) as metadata_in:\n",
    "            img_data, img_metadata = json.load(img_in), json.load(metadata_in)\n",
    "        assert len(img_data) == len(img_metadata)\n",
    "\n",
    "        get_name = lambda line: line.strip().split(\",\")[0]\n",
    "        with open(classes_path) as fin:\n",
    "            self.class2id = {get_name(line): idx for idx, line in enumerate(fin)}\n",
    "\n",
    "        self.samples = []\n",
    "        for img, objs_data in zip(img_data, img_metadata):\n",
    "            assert img[\"image_id\"] == objs_data[\"image_id\"]\n",
    "            img_path = path_images / \"/\".join(img[\"url\"].split(\"/\")[-2:])\n",
    "\n",
    "            objs = self._filter_objs(img, objs_data[\"objects\"])\n",
    "            if len(objs) > 2:\n",
    "                self.samples.append((img_path, objs))\n",
    "\n",
    "        self.id2class = {v: k for k, v in self.class2id.items()}\n",
    "        self.transform = transform\n",
    "        self.max_objects = max_objects\n",
    "        self.resizer = transforms.Resize(size=(image_size, image_size))\n",
    "\n",
    "    def _filter_objs(self, img, objs):\n",
    "        filtered_objs = []\n",
    "        for obj in objs:\n",
    "            o_name = next(filter(lambda x: x in self.class2id, obj[\"names\"]), None)\n",
    "            if o_name is None:\n",
    "                continue\n",
    "            obj[\"names\"] = [o_name]\n",
    "\n",
    "            x, y, h, w = obj[\"x\"], obj[\"y\"], obj[\"h\"], obj[\"w\"]\n",
    "            img_area = img[\"width\"] * img[\"height\"]\n",
    "            obj_area = (x + w) * (y + h)\n",
    "            is_big = obj_area / img_area > 0.01 and w > 1 and h > 1\n",
    "            if is_big:\n",
    "                filtered_objs.append(obj)\n",
    "        return filtered_objs\n",
    "\n",
    "    def _extract_object(self, image, obj_data):\n",
    "        label = self.class2id[obj_data[\"names\"][0]]\n",
    "        y, x, h, w = obj_data[\"y\"], obj_data[\"x\"], obj_data[\"h\"], obj_data[\"w\"]\n",
    "        obj = self.resizer(crop(image, y, x, h, w))\n",
    "        return obj, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_and_transform(self, img_path):\n",
    "        image = pil_loader(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for random distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainVisualGenomeDatasetRandomDistractors(VisualGenomeDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TrainVisualGenomeDatasetRandomDistractors, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, bboxes = self.samples[index]\n",
    "        image = self._load_and_transform(img_path)\n",
    "\n",
    "        cropped_obj, label = self._extract_object(image, bboxes[0])\n",
    "\n",
    "        cropped_objs, labels = [cropped_obj], [label]\n",
    "        distractors = random.sample(self.samples, k=self.max_objects - 1)\n",
    "        for img_path, bboxes in distractors:\n",
    "            image = self._load_and_transform(img_path)\n",
    "\n",
    "            cropped_obj, label = self._extract_object(image, bboxes[0])\n",
    "            labels.append(label)\n",
    "            cropped_objs.append(cropped_obj)\n",
    "\n",
    "        game_input = torch.stack(cropped_objs)\n",
    "        labels = torch.Tensor(labels)\n",
    "\n",
    "        mask = torch.ones(self.max_objects).bool()\n",
    "        game_labels = torch.arange(self.max_objects)\n",
    "        baseline = torch.Tensor([1 / self.max_objects])\n",
    "        aux_input = {\"mask\": mask, \"game_labels\": game_labels, \"baseline\": baseline}\n",
    "        return game_input, labels, torch.zeros(1), aux_input\n",
    "\n",
    "\n",
    "class TestVisualGenomeDatasetRandomDistractors(VisualGenomeDataset, IterableDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TestVisualGenomeDatasetRandomDistractors, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.curr_idx = 0\n",
    "        self.curr_obj_idx = 0\n",
    "\n",
    "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "        rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        per_gpu = int(floor(len(self.samples) / float(world_size)))\n",
    "\n",
    "        iter_start = per_gpu * rank\n",
    "        iter_end = iter_start + per_gpu\n",
    "\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info:  # num_workers is > 0\n",
    "            per_worker = int(floor(per_gpu / worker_info.num_workers))\n",
    "            iter_start = iter_start + worker_info.id * per_worker\n",
    "            iter_end = iter_start + per_worker\n",
    "\n",
    "        self.samples = self.samples[iter_start:iter_end]\n",
    "        return self\n",
    "\n",
    "    def _load_new_sample(self):\n",
    "        img_path, obj_data = self.samples[self.curr_idx]\n",
    "        self.curr_img = self._load_and_transform(img_path)\n",
    "        self.curr_obj_data = obj_data\n",
    "\n",
    "    def __next__(self):\n",
    "        self._load_new_sample()\n",
    "        max_obj_idx = min(self.max_objects, len(self.curr_obj_data))\n",
    "\n",
    "        if self.curr_obj_idx >= max_obj_idx:\n",
    "            self.curr_obj_idx = 0\n",
    "            self.curr_idx += 1\n",
    "            if self.curr_idx >= len(self.samples):\n",
    "                raise StopIteration\n",
    "            self._load_new_sample()\n",
    "\n",
    "        img = self.curr_img\n",
    "        obj_data = self.curr_obj_data[self.curr_obj_idx]\n",
    "        obj, label = self._extract_object(img, obj_data)\n",
    "        self.curr_obj_idx += 1\n",
    "\n",
    "        cropped_objs, labels = [obj], [label]\n",
    "        distractors = random.sample(self.samples, k=self.max_objects - 1)\n",
    "        for img_path, bboxes in distractors:\n",
    "            image = self._load_and_transform(img_path)\n",
    "\n",
    "            cropped_obj, label = self._extract_object(image, bboxes[0])\n",
    "            labels.append(label)\n",
    "            cropped_objs.append(cropped_obj)\n",
    "\n",
    "        game_input = torch.stack(cropped_objs)\n",
    "        labels = torch.Tensor(labels)\n",
    "\n",
    "        mask = torch.ones(self.max_objects).bool()\n",
    "        game_labels = torch.arange(self.max_objects)\n",
    "        baseline = torch.Tensor([1 / self.max_objects])\n",
    "        aux_input = {\"mask\": mask, \"game_labels\": game_labels, \"baseline\": baseline}\n",
    "        return game_input, label, torch.zeros(1), aux_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for contextual distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualGenomeDatasetCtxDistractors(VisualGenomeDataset, torch.utils.data.Dataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(VisualGenomeDatasetCtxDistractors, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, bboxes = self.samples[index]\n",
    "        image = self._load_and_transform(img_path)\n",
    "\n",
    "        cropped_objs, labels = [], []\n",
    "        for obj in bboxes[: min(self.max_objects, len(bboxes))]:\n",
    "            cropped_obj, label = self._extract_object(image, obj)\n",
    "            labels.append(label)\n",
    "            cropped_objs.append(cropped_obj)\n",
    "\n",
    "        agent_input = torch.stack(cropped_objs)\n",
    "        labels = torch.Tensor(labels)\n",
    "        return agent_input, labels\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    inp, lab = [], []\n",
    "    for x, l in batch:\n",
    "        inp.append(x)\n",
    "        lab.append(l)\n",
    "\n",
    "    inp = torch.nn.utils.rnn.pad_sequence(inp, batch_first=True, padding_value=-1)\n",
    "    lab = torch.nn.utils.rnn.pad_sequence(lab, batch_first=True, padding_value=-1)\n",
    "\n",
    "    mask = inp[:, :, 0, 0, 0] != -1\n",
    "    baseline = 1 / mask.int().sum(-1)\n",
    "    bsz, max_objs = inp.shape[:2]\n",
    "    game_labels = torch.arange(max_objs).repeat(bsz, 1)\n",
    "\n",
    "    aux_input = {\"mask\": mask, \"game_labels\": game_labels, \"baseline\": baseline}\n",
    "    return inp, lab, None, aux_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    image_dir: str = \"/datasets01/VisualGenome1.2/061517/\",\n",
    "    metadata_dir: str = \"/private/home/rdessi/visual_genome/train_val_test_split_clean\",\n",
    "    batch_size: int = 32,\n",
    "    split: str = \"train\",\n",
    "    image_size: int = 32,\n",
    "    max_objects: int = 20,\n",
    "    random_distractors: bool = False,\n",
    "    seed: int = 111,\n",
    "):\n",
    "    collate_fn = None\n",
    "\n",
    "    kwargs = {\n",
    "        \"image_dir\": image_dir,\n",
    "        \"metadata_dir\": metadata_dir,\n",
    "        \"split\": split,\n",
    "        \"max_objects\": max_objects,\n",
    "        \"image_size\": image_size,\n",
    "    }\n",
    "    if random_distractors:\n",
    "        if split == \"test\":\n",
    "            dataset = TestVisualGenomeDatasetRandomDistractors(**kwargs)\n",
    "        else:\n",
    "            dataset = TrainVisualGenomeDatasetRandomDistractors(**kwargs)\n",
    "\n",
    "    else:\n",
    "        collate_fn = collate\n",
    "        dataset = VisualGenomeDatasetCtxDistractors(**kwargs)\n",
    "\n",
    "    is_iterable_dataset = random_distractors and split == \"test\"\n",
    "    sampler = None\n",
    "    if dist.is_initialized():\n",
    "        if not is_iterable_dataset:\n",
    "            shuffle = split != \"test\"\n",
    "            sampler = DistributedSampler(\n",
    "                dataset, shuffle=shuffle, drop_last=True, seed=seed\n",
    "            )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=6,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=(not is_iterable_dataset and sampler is None),\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    batch_size: 32,\n",
    "    split: \"train\",\n",
    "    image_size: 32,\n",
    "    max_objects: 20,\n",
    "    random_distractors: False,\n",
    "    seed: 111,\n",
    "}\n",
    "\n",
    "dloader = get_dataloader(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=4,\n",
    "    num_workers=6,\n",
    "    collate_fn=collate,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "id2class = dl.dataset.id2class\n",
    "\n",
    "idx = 0\n",
    "last_obj = 8\n",
    "for inp, labels, _, aux_input in dl:\n",
    "    img = inp[0][0]\n",
    "    all_objs = torch.cat([img.permute(1, 2, 0) for img in inp[0][:last_obj]], dim=1)\n",
    "\n",
    "    lab = labels[0].tolist()\n",
    "    title = \" \".join([id2class[elem] for elem in lab[:last_obj] if elem >= 0])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.imshow(all_objs.numpy())\n",
    "    plt.show()\n",
    "\n",
    "    idx += 1\n",
    "    if idx == 9:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
