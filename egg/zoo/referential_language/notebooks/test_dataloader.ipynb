{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import crop, resize\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path: str) -> Image.Image:\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "class GaussianBlur:\n",
    "    def __init__(self, sigma=[0.1, 2.0]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, img):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        return img.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "\n",
    "\n",
    "class ImageTransformation:\n",
    "    def __init__(self, use_augmentation: bool = False):\n",
    "        transformations = [transforms.ToTensor()]\n",
    "        if use_augmentation:\n",
    "            augmentations = [\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)], p=0.8\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.RandomApply([GaussianBlur([0.1, 2.0])], p=0.5),\n",
    "            ]\n",
    "            transformations = augmentations + transformations\n",
    "\n",
    "        self.transform = transforms.Compose(transformations)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x)\n",
    "\n",
    "\n",
    "class VisualGenomeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        metadata_dir: str,\n",
    "        classes_path: str = \"/private/home/rdessi/EGG/egg/zoo/referential_language/utils/classes_1600.txt\",\n",
    "        split: str = \"train\",\n",
    "        transform: Callable = None,\n",
    "        max_objects=10,\n",
    "        image_size=64,\n",
    "    ):\n",
    "        assert max_objects >= 3\n",
    "        path_images = Path(image_dir)\n",
    "        path_metadata = Path(metadata_dir) / f\"{split}_objects.json\"\n",
    "        path_image_data = Path(metadata_dir) / f\"{split}_image_data.json\"\n",
    "\n",
    "        with open(path_image_data) as img_in, open(path_metadata) as metadata_in:\n",
    "            img_data, object_data = json.load(img_in), json.load(metadata_in)\n",
    "        assert len(img_data) == len(object_data)\n",
    "\n",
    "        self.class2id = {}\n",
    "        idx = 0\n",
    "        with open(classes_path) as f:\n",
    "            for line in f:\n",
    "                names = line.strip().split(\",\")\n",
    "                for name in names:\n",
    "                    self.class2id[name] = idx\n",
    "                    idx += 1\n",
    "\n",
    "        object_dict = {}\n",
    "        for object_item in object_data:\n",
    "            object_dict[object_item[\"image_id\"]] = object_item\n",
    "\n",
    "        self.samples = []\n",
    "        for img_item in img_data:\n",
    "            img_id = img_item[\"image_id\"]\n",
    "            object_item = object_dict[img_id]\n",
    "\n",
    "            img_path = path_images / \"/\".join(img_item[\"url\"].split(\"/\")[-2:])\n",
    "\n",
    "            self.samples.append((img_path, img_id, object_item[\"objects\"]))\n",
    "\n",
    "        self.id2class = {v: k for k, v in self.class2id.items()}\n",
    "        self.transform = transform\n",
    "        self.max_objects = max_objects\n",
    "        self.resizer = transforms.Resize(size=(image_size, image_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_and_transform(self, img_path):\n",
    "        image = pil_loader(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, img_id, obj_list = self.samples[index]\n",
    "\n",
    "        sender_image = self._load_and_transform(img_path)\n",
    "        recv_image = self._load_and_transform(img_path)\n",
    "\n",
    "        sender_objs, labels, recv_objs = [], [], []\n",
    "        bboxes, bboxes_ids = [], []\n",
    "        for obj_item in obj_list[: min(self.max_objects, len(obj_list))]:\n",
    "            x, y, w, h = obj_item[\"x\"], obj_item[\"y\"], obj_item[\"w\"], obj_item[\"h\"]\n",
    "            bboxes.append(torch.IntTensor([x, y, w, h]))\n",
    "            bboxes_ids.append(torch.Tensor([obj_item[\"object_id\"]]))\n",
    "\n",
    "            sender_obj = self.resizer(crop(sender_image, y, x, h, w))\n",
    "            recv_obj = self.resizer(crop(recv_image, y, x, h, w))\n",
    "\n",
    "            sender_objs.append(sender_obj)\n",
    "            recv_objs.append(recv_obj)\n",
    "\n",
    "            label = next(filter(lambda n: n in self.class2id, obj_item[\"names\"]), None)\n",
    "            assert label is not None\n",
    "            labels.append(self.class2id[label])\n",
    "\n",
    "        sender_input = torch.stack(sender_objs)\n",
    "        recv_input = torch.stack(recv_objs)\n",
    "        labels = torch.Tensor(labels)\n",
    "        aux = {\n",
    "            \"sender_image\": resize(sender_image, size=(128, 128)),\n",
    "            \"recv_image\": resize(recv_image, size=(128, 128)),\n",
    "            \"image_ids\": torch.Tensor([img_id]).int(),\n",
    "            \"bboxes\": torch.stack(bboxes),\n",
    "            \"image_sizes\": torch.Tensor([sender_image.shape]).int(),\n",
    "            \"bboxes_ids\": torch.stack(bboxes_ids),\n",
    "        }\n",
    "\n",
    "        return sender_input, labels, recv_input, aux\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    sender_input, labels, recv_input = [], [], []\n",
    "    aux_input = defaultdict(list)\n",
    "    for obj_sender, label, obj_recv, aux in batch:\n",
    "        sender_input.append(obj_sender)\n",
    "        labels.append(label)\n",
    "        recv_input.append(obj_recv)\n",
    "\n",
    "        for k, v in aux.items():\n",
    "            aux_input[k].append(v)\n",
    "\n",
    "    def pad(elem):\n",
    "        if isinstance(elem, list):\n",
    "            return pad_sequence(elem, batch_first=True, padding_value=-1)\n",
    "        elif isinstance(elem, dict):\n",
    "            return {k: pad(v) for k, v in elem.items()}\n",
    "        elif isinstance(elem, torch.Tensor):\n",
    "            return elem\n",
    "        else:\n",
    "            raise RuntimeError(\"Cannot pad elem of type {type(elem)}\")\n",
    "\n",
    "    sender_input = pad(sender_input)\n",
    "    recv_input = pad(recv_input)\n",
    "    labels = pad(labels)\n",
    "    aux_input = pad(aux_input)\n",
    "    aux_input[\"mask\"] = sender_input[:, :, 0, 0, 0] != -1\n",
    "\n",
    "    return sender_input, labels, recv_input, aux_input\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    image_dir: str = \"/private/home/rdessi/visual_genome\",\n",
    "    metadata_dir: str = \"/private/home/rdessi/visual_genome/filtered_splits\",\n",
    "    batch_size: int = 32,\n",
    "    split: str = \"train\",\n",
    "    image_size: int = 32,\n",
    "    max_objects: int = 20,\n",
    "    use_augmentation: bool = False,\n",
    "    seed: int = 111,\n",
    "):\n",
    "    ds = VisualGenomeDataset(\n",
    "        image_dir=image_dir,\n",
    "        metadata_dir=metadata_dir,\n",
    "        split=split,\n",
    "        transform=ImageTransformation(use_augmentation),\n",
    "        max_objects=max_objects,\n",
    "        image_size=image_size,\n",
    "    )\n",
    "\n",
    "    sampler = None\n",
    "    if dist.is_initialized():\n",
    "        sampler = DistributedSampler(\n",
    "            ds, shuffle=(split != \"test\"), drop_last=True, seed=seed\n",
    "        )\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=6,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collate,\n",
    "        shuffle=(sampler is None and split != \"test\"),\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_dataloader(split=\"test\", batch_size=8, use_augmentation=False, image_size=128, seed=111)\n",
    "id2class = dl.dataset.id2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/private/home/rdessi/miniconda3/envs/egg/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/private/home/rdessi/miniconda3/envs/egg/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-8-74d08be14cac>\", line 153, in collate\n    aux_input = pad(aux_input)\n  File \"<ipython-input-8-74d08be14cac>\", line 144, in pad\n    return {k: pad(v) for k, v in elem.items()}\n  File \"<ipython-input-8-74d08be14cac>\", line 144, in <dictcomp>\n    return {k: pad(v) for k, v in elem.items()}\n  File \"<ipython-input-8-74d08be14cac>\", line 142, in pad\n    return pad_sequence(elem, batch_first=True, padding_value=-1)\n  File \"/private/home/rdessi/miniconda3/envs/egg/lib/python3.7/site-packages/torch/nn/utils/rnn.py\", line 371, in pad_sequence\n    out_tensor[i, :length, ...] = tensor\nRuntimeError: The expanded size of the tensor (250) must match the existing size (333) at non-singleton dimension 1.  Target sizes: [3, 250, 500].  Tensor sizes: [3, 333, 500]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-057552e59c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msender_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecv_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/egg/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/egg/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/egg/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/egg/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/private/home/rdessi/miniconda3/envs/egg/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/private/home/rdessi/miniconda3/envs/egg/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-8-74d08be14cac>\", line 153, in collate\n    aux_input = pad(aux_input)\n  File \"<ipython-input-8-74d08be14cac>\", line 144, in pad\n    return {k: pad(v) for k, v in elem.items()}\n  File \"<ipython-input-8-74d08be14cac>\", line 144, in <dictcomp>\n    return {k: pad(v) for k, v in elem.items()}\n  File \"<ipython-input-8-74d08be14cac>\", line 142, in pad\n    return pad_sequence(elem, batch_first=True, padding_value=-1)\n  File \"/private/home/rdessi/miniconda3/envs/egg/lib/python3.7/site-packages/torch/nn/utils/rnn.py\", line 371, in pad_sequence\n    out_tensor[i, :length, ...] = tensor\nRuntimeError: The expanded size of the tensor (250) must match the existing size (333) at non-singleton dimension 1.  Target sizes: [3, 250, 500].  Tensor sizes: [3, 333, 500]\n"
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(dl):\n",
    "    if batch_id == 2:\n",
    "        break\n",
    "    sender_input, labels, recv_input, aux_input = batch\n",
    "    \n",
    "    # sender input of size bsz X max_objs X 3 X image_size X image_size\n",
    "    for idx in range(sender_input.shape[1]):\n",
    "        if aux_input[\"mask\"][batch_id][idx] == False:\n",
    "            continue\n",
    "        obj_sender = sender_input[batch_id][idx].permute(1, 2, 0)\n",
    "        obj_recv = recv_input[batch_id][idx].permute(1, 2, 0)\n",
    "\n",
    "        sender_img = aux_input[\"sender_image\"][batch_id].permute(1, 2, 0)\n",
    "        recv_img = aux_input[\"recv_image\"][batch_id].permute(1, 2, 0)\n",
    "        lineup = torch.cat([obj_sender, obj_recv, sender_img, recv_img], dim=1)\n",
    "        \n",
    "        label_sender = id2class.get(labels[batch_id][idx].item(), \"None\")\n",
    "        label_recv = id2class.get(labels[batch_id][idx].item(), \"None\")\n",
    "\n",
    "        plt.title(f\"Batch {batch_id}, obj {idx}, label, {label_sender}\")\n",
    "        plt.imshow(lineup.numpy())\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
